\documentclass[twoside,11pt]{homework}

\coursename{ECBM6040 NEURAL NETWORKS\&DEEP LEARNING (Spring 2015)} % DON'T CHANGE THIS

\studname{Shenlong Gu}    % YOUR NAME GOES HERE
\studmail{sg3301@columbia.edu}% YOUR UNI GOES HERE
\hwNo{2}                   % THE HOMEWORK NUMBER GOES HERE
\collab{}   % THE UNI'S OF STUDENTS YOU DISCUSSED WITH

% Uncomment the next line if you want to use \includegraphics.
%\usepackage{graphicx}

\begin{document}

\maketitle

\section*{Problem 1}
1. We get the derivative for the function and set it to be zero and we find: \\
	$\sum\limits_{i=1}^m (y^{i} - Ax^{i}) * (x^i)^T$ = 0. \\
	Solve it and write it into the matrix form, we get: \\ 
	$A = YX^T(XX^T)^{-1}$ \\ \\
		
2. the same as the first question and set the derivative to be zero and we find: \\
	$\lambda * A - \sum\limits_{i=1}^m (y^{i} - Ax^{i}) * (x^i)^T$ = 0 \\
	$A =  YX^T(XX^T + \lambda * I)^{-1}$ \\ \\ 
	
3. We set the derivative of the log maximum likelihood estimate to be zero: (really the same as the first question) \\
	$\sum\limits_{i=1}^m (y^{i} - Ax^{i}) * (x^i)^T$ = 0. \\
	$A = YX^T(XX^T)^{-1}$ \\ \\

4. We set the derivative of log MPE to be zero and we find: \\
	$\lambda \sigma^{2}A - \sum\limits_{i=1}^m (y^{i} - Ax^{i}) * (x^i)^T$ = 0 \\
	$A = YX^T(XX^T + \lambda \sigma^{2}I)^{-1}$ \\ \\
5.  The result of the question 1 and 3 is the same. It is just a MLE interpretation and a loss function interpretation which lead to the same result. The result of question 2 and 4 is different by a $\sigma^{2}$. When $\sigma = 1$, the results are same, just regularization interpretation and Bayes prior interpretation which also lead to the same result.

\end{document} 
